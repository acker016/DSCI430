{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to `pyspark.sql.DataFrame`s\n",
    "\n",
    "Adapted from [Databrick's tutorial](https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# import pyspark class Row from module sql\n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is spark?\n",
    "\n",
    "* Build for the Hadoop platform\n",
    "* Replacement of MapReduce\n",
    "* Second-generation optimization\n",
    "    * Lazy\n",
    "    * Optimized\n",
    "    * Persistent data structures\n",
    "* Written in scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ok ... so what's Hadoop?\n",
    "\n",
    "* Distributed computing platform\n",
    "* [Used by lots of companies](https://wiki.apache.org/hadoop/PoweredBy)\n",
    "* Becoming an inductry standard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is `pyspark`?\n",
    "\n",
    "* Python interface to spark\n",
    "* Needs a spark session\n",
    "    * `session` $\\leftrightarrow$ spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 0 - Create a spark session\n",
    "\n",
    "* `pyspark` communicates with `spark` through a session\n",
    "* Similar to `sqlalchemy` session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Ops').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview -  `pyspark.DataFrame`\n",
    "\n",
    "* A `DataFrame` is a collection of `Row`s\n",
    "* `Row`s can be distributed over many machines\n",
    "* `spark`\n",
    "    * Hides the messy details\n",
    "    * Optimizes operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a `Row` of data\n",
    "\n",
    "* Use the `Row` class\n",
    "* Pass data using keywords\n",
    "    * key == column name\n",
    "    * value == cell value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(id='123456', name='Computer Science')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "department1 = Row(id='123456', name='Computer Science')\n",
    "department1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpacking a `Row` dictionary\n",
    "\n",
    "* Data is in a row dictionary\n",
    "* Unpack keywords using `**`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(id='789012', name='Mechanical Engineering')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dept2_info = {'id':'789012', 'name':'Mechanical Engineering'}\n",
    "department2 = Row(**dept2_info)\n",
    "department2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpacking a list of row dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=123456, name='Computer Science'),\n",
       " Row(id=789012, name='Mechanical Engineering'),\n",
       " Row(id=345678, name='Theater and Drama'),\n",
       " Row(id=901234, name='Indoor Recreation')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dept_info = [{'id':123456, 'name':'Computer Science'},\n",
    "             {'id':789012, 'name':'Mechanical Engineering'},\n",
    "             {'id':345678, 'name':'Theater and Drama'},\n",
    "             {'id':901234, 'name':'Indoor Recreation'}]\n",
    "\n",
    "dept_rows = [Row(**r) for r in dept_info]\n",
    "dept_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access `Row` content with column attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[123456, 789012, 345678, 901234]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[dept.id for dept in dept_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Computer Science',\n",
       " 'Mechanical Engineering',\n",
       " 'Theater and Drama',\n",
       " 'Indoor Recreation']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[dept.name for dept in dept_rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a `pyspark.DataFrame`\n",
    "\n",
    "* A `DataFrame` is a collection of `Row`s\n",
    "* Create with spark.createDataFrame\n",
    "* Need to have a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, name: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(dept_rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to think about a `pyspark.DataFrame`\n",
    "\n",
    "<img src=\"./img/pyspark_df.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - `filter` and `collect`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o47.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost, executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:170)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:108)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:164)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3258)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3255)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3255)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:170)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:108)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:164)\r\n\t... 32 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-b458376edd67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m output = (df\n\u001b[1;32m----> 2\u001b[1;33m             \u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m             .collect())\n\u001b[0;32m      4\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    531\u001b[0m         \"\"\"\n\u001b[0;32m    532\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o47.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost, executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:170)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:108)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:164)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3258)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3255)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3255)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:170)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:108)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:164)\r\n\t... 32 more\r\n"
     ]
    }
   ],
   "source": [
    "output = (df\n",
    "            .filter(df.name.startswith('C'))\n",
    "            .collect())\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is `pyspark` so slow\n",
    "\n",
    "* Optimized for \n",
    "    * Distributed computation\n",
    "    * Big data \n",
    "* Not great for\n",
    "    * Local work\n",
    "    * Small data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `filter` and `collect` illustrated\n",
    "\n",
    "<img src=\"./img/pyspark_filter_collect.gif\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a `csv` file with `spark.read.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Id: string, name: string, Gender: string, Eye color: string, Race: string, Hair color: string, Height: string, Publisher: string, Skin color: string, Alignment: string, Weight: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heros = spark.read.csv('./data/heroes_information.csv', header=True)\n",
    "heros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Eye color: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Hair color: string (nullable = true)\n",
      " |-- Height: string (nullable = true)\n",
      " |-- Publisher: string (nullable = true)\n",
      " |-- Skin color: string (nullable = true)\n",
      " |-- Alignment: string (nullable = true)\n",
      " |-- Weight: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "heros.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering results in `pyspark.sql`\n",
    "\n",
    "* **Important fact** All `pyspark` queries end in a collection method\n",
    "* **Why?** Data is (possibly) spread across many machines\n",
    "* <font color = \"red\"> **Warning** This might be is *expensive*! Know how much data your are requesting! </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering methods\n",
    "\n",
    "* `collect` returns all values\n",
    "* `take(n)` returns the first `n` values \n",
    "* `sample(n)` returns `n` randomly selected values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the content - `take`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Id='0', name='A-Bomb', Gender='Male', Eye color='yellow', Race='Human', Hair color='No Hair', Height='203.0', Publisher='Marvel Comics', Skin color='-', Alignment='good', Weight='441.0'),\n",
       " Row(Id='1', name='Abe Sapien', Gender='Male', Eye color='blue', Race='Icthyo Sapien', Hair color='No Hair', Height='191.0', Publisher='Dark Horse Comics', Skin color='blue', Alignment='good', Weight='65.0'),\n",
       " Row(Id='2', name='Abin Sur', Gender='Male', Eye color='blue', Race='Ungaran', Hair color='No Hair', Height='185.0', Publisher='DC Comics', Skin color='red', Alignment='good', Weight='90.0'),\n",
       " Row(Id='3', name='Abomination', Gender='Male', Eye color='green', Race='Human / Radiation', Hair color='No Hair', Height='203.0', Publisher='Marvel Comics', Skin color='-', Alignment='bad', Weight='441.0'),\n",
       " Row(Id='4', name='Abraxas', Gender='Male', Eye color='blue', Race='Cosmic Entity', Hair color='Black', Height='-99.0', Publisher='Marvel Comics', Skin color='-', Alignment='bad', Weight='-99.0')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heros.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the content - `sample`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Id='50', name='Atom', Gender='Male', Eye color='-', Race='-', Hair color='-', Height='-99.0', Publisher='DC Comics', Skin color='-', Alignment='good', Weight='-99.0'),\n",
       " Row(Id='106', name='Black Widow', Gender='Female', Eye color='green', Race='Human', Hair color='Auburn', Height='170.0', Publisher='Marvel Comics', Skin color='-', Alignment='good', Weight='59.0'),\n",
       " Row(Id='121', name='Bloodwraith', Gender='Male', Eye color='white', Race='-', Hair color='No Hair', Height='30.5', Publisher='Marvel Comics', Skin color='-', Alignment='bad', Weight='-99.0'),\n",
       " Row(Id='123', name='Blue Beetle', Gender='Male', Eye color='-', Race='-', Hair color='-', Height='-99.0', Publisher='DC Comics', Skin color='-', Alignment='good', Weight='-99.0'),\n",
       " Row(Id='195', name='Cyclops', Gender='Male', Eye color='brown', Race='Mutant', Hair color='Brown', Height='191.0', Publisher='Marvel Comics', Skin color='-', Alignment='good', Weight='88.0'),\n",
       " Row(Id='266', name='Flash IV', Gender='Male', Eye color='yellow', Race='Human', Hair color='Auburn', Height='157.0', Publisher='DC Comics', Skin color='-', Alignment='good', Weight='52.0'),\n",
       " Row(Id='293', name='Gorilla Grodd', Gender='Male', Eye color='yellow', Race='Gorilla', Hair color='Black', Height='198.0', Publisher='DC Comics', Skin color='-', Alignment='bad', Weight='270.0'),\n",
       " Row(Id='365', name='Johann Krauss', Gender='Male', Eye color='-', Race='-', Hair color='-', Height='-99.0', Publisher='Dark Horse Comics', Skin color='-', Alignment='good', Weight='-99.0'),\n",
       " Row(Id='462', name='Mockingbird', Gender='Female', Eye color='blue', Race='Human', Hair color='Blond', Height='175.0', Publisher='Marvel Comics', Skin color='-', Alignment='good', Weight='61.0'),\n",
       " Row(Id='633', name='Stardust', Gender='Male', Eye color='-', Race='-', Hair color='-', Height='-99.0', Publisher='Marvel Comics', Skin color='-', Alignment='good', Weight='-99.0')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heros.sample(fraction=0.01).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting all results with `collect`\n",
    "\n",
    "<font color = \"red\"> **Warning** This might be is *expensive*! Know how much data your are requesting! </font> \n",
    "\n",
    "**The `collect` rule:** `count` before `collect`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heros.collect() # <-- probably don't do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heros.filter(heros['Eye Color'] == 'blue').collect() # <-- better but still might be lots, let's check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heros.filter(heros['Eye Color'] == 'blue').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Did you notice?\n",
    "\n",
    "<img src=\"./img/pyspark_missing_values.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying a `nullValue`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Id='0', name='A-Bomb', Gender='Male', Eye color='yellow', Race='Human', Hair color='No Hair', Height='203.0', Publisher='Marvel Comics', Skin color=None, Alignment='good', Weight='441.0'),\n",
       " Row(Id='1', name='Abe Sapien', Gender='Male', Eye color='blue', Race='Icthyo Sapien', Hair color='No Hair', Height='191.0', Publisher='Dark Horse Comics', Skin color='blue', Alignment='good', Weight='65.0'),\n",
       " Row(Id='2', name='Abin Sur', Gender='Male', Eye color='blue', Race='Ungaran', Hair color='No Hair', Height='185.0', Publisher='DC Comics', Skin color='red', Alignment='good', Weight='90.0'),\n",
       " Row(Id='3', name='Abomination', Gender='Male', Eye color='green', Race='Human / Radiation', Hair color='No Hair', Height='203.0', Publisher='Marvel Comics', Skin color=None, Alignment='bad', Weight='441.0'),\n",
       " Row(Id='4', name='Abraxas', Gender='Male', Eye color='blue', Race='Cosmic Entity', Hair color='Black', Height='-99.0', Publisher='Marvel Comics', Skin color=None, Alignment='bad', Weight='-99.0')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heros = spark.read.csv('./data/heroes_information.csv', header=True, nullValue='-')\n",
    "heros.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Did you notice?\n",
    "\n",
    "<img src=\"./img/pyspark_default_types.png\" width=400>\n",
    "\n",
    "Default type is a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Letting `spark` guess the types\n",
    "\n",
    "Set `inferScheme=True` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Id: int, name: string, Gender: string, Eye color: string, Race: string, Hair color: string, Height: double, Publisher: string, Skin color: string, Alignment: string, Weight: double]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heros = spark.read.csv('./data/heroes_information.csv', header=True, inferSchema=True, nullValue='-')\n",
    "heros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the column types after `inferScheme`\n",
    "\n",
    "In this case, `spark` guessed correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Eye color: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Hair color: string (nullable = true)\n",
      " |-- Height: double (nullable = true)\n",
      " |-- Publisher: string (nullable = true)\n",
      " |-- Skin color: string (nullable = true)\n",
      " |-- Alignment: string (nullable = true)\n",
      " |-- Weight: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "heros.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the content - `take`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Id=0, name='A-Bomb', Gender='Male', Eye color='yellow', Race='Human', Hair color='No Hair', Height=203.0, Publisher='Marvel Comics', Skin color=None, Alignment='good', Weight=441.0),\n",
       " Row(Id=1, name='Abe Sapien', Gender='Male', Eye color='blue', Race='Icthyo Sapien', Hair color='No Hair', Height=191.0, Publisher='Dark Horse Comics', Skin color='blue', Alignment='good', Weight=65.0),\n",
       " Row(Id=2, name='Abin Sur', Gender='Male', Eye color='blue', Race='Ungaran', Hair color='No Hair', Height=185.0, Publisher='DC Comics', Skin color='red', Alignment='good', Weight=90.0),\n",
       " Row(Id=3, name='Abomination', Gender='Male', Eye color='green', Race='Human / Radiation', Hair color='No Hair', Height=203.0, Publisher='Marvel Comics', Skin color=None, Alignment='bad', Weight=441.0),\n",
       " Row(Id=4, name='Abraxas', Gender='Male', Eye color='blue', Race='Cosmic Entity', Hair color='Black', Height=-99.0, Publisher='Marvel Comics', Skin color=None, Alignment='bad', Weight=-99.0)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heros.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicit `schema` specification\n",
    "\n",
    "Format is `add(name, type, nullable?)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Id: int, name: string, Gender: string, Eye color: string, Race: string, Hair color: string, Height: double, Publisher: string, Skin color: string, Alignment: string, Weight: double]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import DoubleType, StringType, IntegerType\n",
    "\n",
    "hero_schema = (StructType()\n",
    "  .add('Id', IntegerType(), False)\n",
    "  .add('name', StringType(), True)\n",
    "  .add('Gender', StringType(), True)\n",
    "  .add('Eye color', StringType(), True)\n",
    "  .add('Race', StringType(), True)\n",
    "  .add('Hair color', StringType(), True)\n",
    "  .add('Height', DoubleType(), True)\n",
    "  .add('Publisher', StringType(), True)\n",
    "  .add('Skin color', StringType(), True)\n",
    "  .add('Alignment', StringType(), True)\n",
    "  .add('Weight', DoubleType(), True))\n",
    "\n",
    "heros = spark.read.csv('./data/heroes_information.csv', header=True, schema=hero_schema, nullValue='-')\n",
    "heros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Id=0, name='A-Bomb', Gender='Male', Eye color='yellow', Race='Human', Hair color='No Hair', Height=203.0, Publisher='Marvel Comics', Skin color=None, Alignment='good', Weight=441.0),\n",
       " Row(Id=1, name='Abe Sapien', Gender='Male', Eye color='blue', Race='Icthyo Sapien', Hair color='No Hair', Height=191.0, Publisher='Dark Horse Comics', Skin color='blue', Alignment='good', Weight=65.0),\n",
       " Row(Id=2, name='Abin Sur', Gender='Male', Eye color='blue', Race='Ungaran', Hair color='No Hair', Height=185.0, Publisher='DC Comics', Skin color='red', Alignment='good', Weight=90.0),\n",
       " Row(Id=3, name='Abomination', Gender='Male', Eye color='green', Race='Human / Radiation', Hair color='No Hair', Height=203.0, Publisher='Marvel Comics', Skin color=None, Alignment='bad', Weight=441.0),\n",
       " Row(Id=4, name='Abraxas', Gender='Male', Eye color='blue', Race='Cosmic Entity', Hair color='Black', Height=-99.0, Publisher='Marvel Comics', Skin color=None, Alignment='bad', Weight=-99.0)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heros.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Exercise 1 </font>\n",
    "\n",
    "Define a `schema` and read in `./data/super_hero_powers.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hero_names: string (nullable = true)\n",
      " |-- Agility: boolean (nullable = true)\n",
      " |-- Accelerated Healing: boolean (nullable = true)\n",
      " |-- Lantern Power Ring: boolean (nullable = true)\n",
      " |-- Dimensional Awareness: boolean (nullable = true)\n",
      " |-- Cold Resistance: boolean (nullable = true)\n",
      " |-- Durability: boolean (nullable = true)\n",
      " |-- Stealth: boolean (nullable = true)\n",
      " |-- Energy Absorption: boolean (nullable = true)\n",
      " |-- Flight: boolean (nullable = true)\n",
      " |-- Danger Sense: boolean (nullable = true)\n",
      " |-- Underwater breathing: boolean (nullable = true)\n",
      " |-- Marksmanship: boolean (nullable = true)\n",
      " |-- Weapons Master: boolean (nullable = true)\n",
      " |-- Power Augmentation: boolean (nullable = true)\n",
      " |-- Animal Attributes: boolean (nullable = true)\n",
      " |-- Longevity: boolean (nullable = true)\n",
      " |-- Intelligence: boolean (nullable = true)\n",
      " |-- Super Strength: boolean (nullable = true)\n",
      " |-- Cryokinesis: boolean (nullable = true)\n",
      " |-- Telepathy: boolean (nullable = true)\n",
      " |-- Energy Armor: boolean (nullable = true)\n",
      " |-- Energy Blasts: boolean (nullable = true)\n",
      " |-- Duplication: boolean (nullable = true)\n",
      " |-- Size Changing: boolean (nullable = true)\n",
      " |-- Density Control: boolean (nullable = true)\n",
      " |-- Stamina: boolean (nullable = true)\n",
      " |-- Astral Travel: boolean (nullable = true)\n",
      " |-- Audio Control: boolean (nullable = true)\n",
      " |-- Dexterity: boolean (nullable = true)\n",
      " |-- Omnitrix: boolean (nullable = true)\n",
      " |-- Super Speed: boolean (nullable = true)\n",
      " |-- Possession: boolean (nullable = true)\n",
      " |-- Animal Oriented Powers: boolean (nullable = true)\n",
      " |-- Weapon-based Powers: boolean (nullable = true)\n",
      " |-- Electrokinesis: boolean (nullable = true)\n",
      " |-- Darkforce Manipulation: boolean (nullable = true)\n",
      " |-- Death Touch: boolean (nullable = true)\n",
      " |-- Teleportation: boolean (nullable = true)\n",
      " |-- Enhanced Senses: boolean (nullable = true)\n",
      " |-- Telekinesis: boolean (nullable = true)\n",
      " |-- Energy Beams: boolean (nullable = true)\n",
      " |-- Magic: boolean (nullable = true)\n",
      " |-- Hyperkinesis: boolean (nullable = true)\n",
      " |-- Jump: boolean (nullable = true)\n",
      " |-- Clairvoyance: boolean (nullable = true)\n",
      " |-- Dimensional Travel: boolean (nullable = true)\n",
      " |-- Power Sense: boolean (nullable = true)\n",
      " |-- Shapeshifting: boolean (nullable = true)\n",
      " |-- Peak Human Condition: boolean (nullable = true)\n",
      " |-- Immortality: boolean (nullable = true)\n",
      " |-- Camouflage: boolean (nullable = true)\n",
      " |-- Element Control: boolean (nullable = true)\n",
      " |-- Phasing: boolean (nullable = true)\n",
      " |-- Astral Projection: boolean (nullable = true)\n",
      " |-- Electrical Transport: boolean (nullable = true)\n",
      " |-- Fire Control: boolean (nullable = true)\n",
      " |-- Projection: boolean (nullable = true)\n",
      " |-- Summoning: boolean (nullable = true)\n",
      " |-- Enhanced Memory: boolean (nullable = true)\n",
      " |-- Reflexes: boolean (nullable = true)\n",
      " |-- Invulnerability: boolean (nullable = true)\n",
      " |-- Energy Constructs: boolean (nullable = true)\n",
      " |-- Force Fields: boolean (nullable = true)\n",
      " |-- Self-Sustenance: boolean (nullable = true)\n",
      " |-- Anti-Gravity: boolean (nullable = true)\n",
      " |-- Empathy: boolean (nullable = true)\n",
      " |-- Power Nullifier: boolean (nullable = true)\n",
      " |-- Radiation Control: boolean (nullable = true)\n",
      " |-- Psionic Powers: boolean (nullable = true)\n",
      " |-- Elasticity: boolean (nullable = true)\n",
      " |-- Substance Secretion: boolean (nullable = true)\n",
      " |-- Elemental Transmogrification: boolean (nullable = true)\n",
      " |-- Technopath/Cyberpath: boolean (nullable = true)\n",
      " |-- Photographic Reflexes: boolean (nullable = true)\n",
      " |-- Seismic Power: boolean (nullable = true)\n",
      " |-- Animation: boolean (nullable = true)\n",
      " |-- Precognition: boolean (nullable = true)\n",
      " |-- Mind Control: boolean (nullable = true)\n",
      " |-- Fire Resistance: boolean (nullable = true)\n",
      " |-- Power Absorption: boolean (nullable = true)\n",
      " |-- Enhanced Hearing: boolean (nullable = true)\n",
      " |-- Nova Force: boolean (nullable = true)\n",
      " |-- Insanity: boolean (nullable = true)\n",
      " |-- Hypnokinesis: boolean (nullable = true)\n",
      " |-- Animal Control: boolean (nullable = true)\n",
      " |-- Natural Armor: boolean (nullable = true)\n",
      " |-- Intangibility: boolean (nullable = true)\n",
      " |-- Enhanced Sight: boolean (nullable = true)\n",
      " |-- Molecular Manipulation: boolean (nullable = true)\n",
      " |-- Heat Generation: boolean (nullable = true)\n",
      " |-- Adaptation: boolean (nullable = true)\n",
      " |-- Gliding: boolean (nullable = true)\n",
      " |-- Power Suit: boolean (nullable = true)\n",
      " |-- Mind Blast: boolean (nullable = true)\n",
      " |-- Probability Manipulation: boolean (nullable = true)\n",
      " |-- Gravity Control: boolean (nullable = true)\n",
      " |-- Regeneration: boolean (nullable = true)\n",
      " |-- Light Control: boolean (nullable = true)\n",
      " |-- Echolocation: boolean (nullable = true)\n",
      " |-- Levitation: boolean (nullable = true)\n",
      " |-- Toxin and Disease Control: boolean (nullable = true)\n",
      " |-- Banish: boolean (nullable = true)\n",
      " |-- Energy Manipulation: boolean (nullable = true)\n",
      " |-- Heat Resistance: boolean (nullable = true)\n",
      " |-- Natural Weapons: boolean (nullable = true)\n",
      " |-- Time Travel: boolean (nullable = true)\n",
      " |-- Enhanced Smell: boolean (nullable = true)\n",
      " |-- Illusions: boolean (nullable = true)\n",
      " |-- Thirstokinesis: boolean (nullable = true)\n",
      " |-- Hair Manipulation: boolean (nullable = true)\n",
      " |-- Illumination: boolean (nullable = true)\n",
      " |-- Omnipotent: boolean (nullable = true)\n",
      " |-- Cloaking: boolean (nullable = true)\n",
      " |-- Changing Armor: boolean (nullable = true)\n",
      " |-- Power Cosmic: boolean (nullable = true)\n",
      " |-- Biokinesis: boolean (nullable = true)\n",
      " |-- Water Control: boolean (nullable = true)\n",
      " |-- Radiation Immunity: boolean (nullable = true)\n",
      " |-- Vision - Telescopic: boolean (nullable = true)\n",
      " |-- Toxin and Disease Resistance: boolean (nullable = true)\n",
      " |-- Spatial Awareness: boolean (nullable = true)\n",
      " |-- Energy Resistance: boolean (nullable = true)\n",
      " |-- Telepathy Resistance: boolean (nullable = true)\n",
      " |-- Molecular Combustion: boolean (nullable = true)\n",
      " |-- Omnilingualism: boolean (nullable = true)\n",
      " |-- Portal Creation: boolean (nullable = true)\n",
      " |-- Magnetism: boolean (nullable = true)\n",
      " |-- Mind Control Resistance: boolean (nullable = true)\n",
      " |-- Plant Control: boolean (nullable = true)\n",
      " |-- Sonar: boolean (nullable = true)\n",
      " |-- Sonic Scream: boolean (nullable = true)\n",
      " |-- Time Manipulation: boolean (nullable = true)\n",
      " |-- Enhanced Touch: boolean (nullable = true)\n",
      " |-- Magic Resistance: boolean (nullable = true)\n",
      " |-- Invisibility: boolean (nullable = true)\n",
      " |-- Sub-Mariner: boolean (nullable = true)\n",
      " |-- Radiation Absorption: boolean (nullable = true)\n",
      " |-- Intuitive aptitude: boolean (nullable = true)\n",
      " |-- Vision - Microscopic: boolean (nullable = true)\n",
      " |-- Melting: boolean (nullable = true)\n",
      " |-- Wind Control: boolean (nullable = true)\n",
      " |-- Super Breath: boolean (nullable = true)\n",
      " |-- Wallcrawling: boolean (nullable = true)\n",
      " |-- Vision - Night: boolean (nullable = true)\n",
      " |-- Vision - Infrared: boolean (nullable = true)\n",
      " |-- Grim Reaping: boolean (nullable = true)\n",
      " |-- Matter Absorption: boolean (nullable = true)\n",
      " |-- The Force: boolean (nullable = true)\n",
      " |-- Resurrection: boolean (nullable = true)\n",
      " |-- Terrakinesis: boolean (nullable = true)\n",
      " |-- Vision - Heat: boolean (nullable = true)\n",
      " |-- Vitakinesis: boolean (nullable = true)\n",
      " |-- Radar Sense: boolean (nullable = true)\n",
      " |-- Qwardian Power Ring: boolean (nullable = true)\n",
      " |-- Weather Control: boolean (nullable = true)\n",
      " |-- Vision - X-Ray: boolean (nullable = true)\n",
      " |-- Vision - Thermal: boolean (nullable = true)\n",
      " |-- Web Creation: boolean (nullable = true)\n",
      " |-- Reality Warping: boolean (nullable = true)\n",
      " |-- Odin Force: boolean (nullable = true)\n",
      " |-- Symbiote Costume: boolean (nullable = true)\n",
      " |-- Speed Force: boolean (nullable = true)\n",
      " |-- Phoenix Force: boolean (nullable = true)\n",
      " |-- Molecular Dissipation: boolean (nullable = true)\n",
      " |-- Vision - Cryo: boolean (nullable = true)\n",
      " |-- Omnipresent: boolean (nullable = true)\n",
      " |-- Omniscient: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import BooleanType, StringType \n",
    "\n",
    "df_heros = spark.read.csv('./data/super_hero_powers.csv', header=True, inferSchema=True, nullValue='-')\n",
    "df_heros.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pyspark.sql` queries are like `sqlalchemy` queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter, group, and aggregate (categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Eye color='grey', count=6),\n",
       " Row(Eye color='green', count=30),\n",
       " Row(Eye color='yellow', count=16),\n",
       " Row(Eye color='bown', count=1),\n",
       " Row(Eye color=None, count=121)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(heros\n",
    "     .where(heros.Gender == 'Male')\n",
    "     .groupby(heros['Eye color'])\n",
    "     .count()\n",
    "     .take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group by multiple and aggregate (categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Eye color='yellow (without irises)', Gender=None, count=1),\n",
       " Row(Eye color='green', Gender='Male', count=30),\n",
       " Row(Eye color='violet', Gender='Female', count=2),\n",
       " Row(Eye color='hazel', Gender='Female', count=3),\n",
       " Row(Eye color='blue', Gender='Male', count=143)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(heros\n",
    "     .groupby(heros['Eye color'], heros.Gender)\n",
    "     .count()\n",
    "     .take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Exercise 2 </font>\n",
    "    \n",
    "Perform `pyspark.sql` queries to answer each of the following questions.\n",
    "\n",
    "1. How many heroes have both Super Strength and Super Speed?\n",
    "2. How many heroes have names that start with the word *Black*\n",
    "3. Are heroes with Agility more likely to have Stealth?\n",
    "4. What fraction of all heroes that can fly also have Super Strength?\n",
    "5. Consider heroes that have names that contain `\"girl\"`, `\"boy\"`, `\"woman\"`, or `\"man\"`.  Compute the following ratio\n",
    "\n",
    "$$\\frac{N(\\text{boy or man})}{N(\\text{girl or woman}}$$\n",
    "\n",
    "**Hint:** You will need to use some combination of `where`, `group_by`, and `count` for each part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "## Creating rows from list of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Row class\n",
    "\n",
    "* Pass `Row` the columns names\n",
    "* Creates a specialized `Row` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Row(firstName, lastName, email, salary)>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Employee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\n",
    "Employee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a `Employee` instance\n",
    "\n",
    "* Pass the data to `Employee` to make a row\n",
    "* Order matters ... use the same order as names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(firstName='michael', lastName='armbrust', email='no-reply@berkeley.edu', salary=100000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Employee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\n",
    "employee1 = Employee('michael', 'armbrust', 'no-reply@berkeley.edu', 100000)\n",
    "employee1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpacking a data list\n",
    "\n",
    "* Suppose the data is in a list/tuple.\n",
    "* Use sequence unpacking with `*`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('xiangrui', 'meng', 'no-reply@stanford.edu', 120000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empl2_info = ('xiangrui', 'meng', 'no-reply@stanford.edu', 120000)\n",
    "empl2_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(firstName='xiangrui', lastName='meng', email='no-reply@stanford.edu', salary=120000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employee2 = Employee(*empl2_info)\n",
    "employee2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpacking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(firstName='michael', lastName='armbrust', email='no-reply@berkeley.edu', salary=100000),\n",
       " Row(firstName='xiangrui', lastName='meng', email='no-reply@stanford.edu', salary=120000),\n",
       " Row(firstName='matei', lastName=None, email='no-reply@waterloo.edu', salary=140000),\n",
       " Row(firstName=None, lastName='wendell', email='no-reply@berkeley.edu', salary=160000)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Employees\n",
    "Employee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\n",
    "employees = [('michael', 'armbrust', 'no-reply@berkeley.edu', 100000),\n",
    "             ('xiangrui', 'meng', 'no-reply@stanford.edu', 120000),\n",
    "             ('matei', None, 'no-reply@waterloo.edu', 140000),\n",
    "             (None, 'wendell', 'no-reply@berkeley.edu', 160000)]\n",
    "emp_rows = [Employee(*r) for r in employees]\n",
    "emp_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
